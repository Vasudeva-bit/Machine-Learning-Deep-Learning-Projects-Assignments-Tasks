{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aesthetic Value of Image (Prediction).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XIM23fbHtMg",
        "outputId": "6ca0406f-87a7-46ff-c88b-801f9b16c6f3"
      },
      "source": [
        "# Mounting data from google drive.!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQPEWa80Q3EV"
      },
      "source": [
        "!unzip drive/MyDrive/petfinder-pawpularity-score.zip > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BlBaCY9RE9r"
      },
      "source": [
        "# Importing all packages needed in the project.\n",
        "import tensorflow as tf\n",
        "# import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import functools\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTLLIFxNRHnR",
        "outputId": "dac16f2b-17b0-4447-ccd7-bf752690e299"
      },
      "source": [
        "# Let us know the version that we are going to work on!!\n",
        "print('tensorflow version: ',tf.__version__)\n",
        "# print('tensorflow_hub: ', hub.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow version:  2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDRXdBgURJ74",
        "outputId": "806d7fc0-9518-4ed2-9463-b5c4077590cf"
      },
      "source": [
        "# We shall check our physical devies connected to the runtime (checking the GPU connection)..!\n",
        "print('GPU availabity status :', 'Availabe' if tf.config.list_physical_devices(\"GPU\") else 'Not available') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU availabity status : Availabe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3m1zqDBgVsid",
        "outputId": "3927f91d-a97d-402c-d42b-c21c478ccb1d"
      },
      "source": [
        "train_csv = pd.read_csv(\"train.csv\")\n",
        "train_csv.head()\n",
        "test_csv = pd.read_csv(\"test.csv\")\n",
        "test_csv.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Subject Focus</th>\n",
              "      <th>Eyes</th>\n",
              "      <th>Face</th>\n",
              "      <th>Near</th>\n",
              "      <th>Action</th>\n",
              "      <th>Accessory</th>\n",
              "      <th>Group</th>\n",
              "      <th>Collage</th>\n",
              "      <th>Human</th>\n",
              "      <th>Occlusion</th>\n",
              "      <th>Info</th>\n",
              "      <th>Blur</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4128bae22183829d2b5fea10effdb0c3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43a2262d7738e3d420d453815151079e</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4e429cead1848a298432a0acad014c9d</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8f49844c382931444e68dffbe20228f4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Id  Subject Focus  Eyes  ...  Occlusion  Info  Blur\n",
              "0  4128bae22183829d2b5fea10effdb0c3              1     0  ...          1     0     1\n",
              "1  43a2262d7738e3d420d453815151079e              0     1  ...          0     0     0\n",
              "2  4e429cead1848a298432a0acad014c9d              0     0  ...          1     1     1\n",
              "3  80bc3ccafcc51b66303c2c263aa38486              1     0  ...          0     1     0\n",
              "4  8f49844c382931444e68dffbe20228f4              1     1  ...          1     1     0\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTg6H12eV3uk",
        "outputId": "c68231f7-be70-4cb7-d59d-aec0adcaa27a"
      },
      "source": [
        "Train_Filenames = ['/content/train/'+fname+'.jpg' for fname in train_csv.Id]\n",
        "# Let's have a look at the first 10 image file names\n",
        "Train_Filenames[:10]\n",
        "Test_Filenames = ['/content/test/'+fname+'.jpg' for fname in test_csv.Id]\n",
        "Test_Filenames[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/test/4128bae22183829d2b5fea10effdb0c3.jpg',\n",
              " '/content/test/43a2262d7738e3d420d453815151079e.jpg',\n",
              " '/content/test/4e429cead1848a298432a0acad014c9d.jpg',\n",
              " '/content/test/80bc3ccafcc51b66303c2c263aa38486.jpg',\n",
              " '/content/test/8f49844c382931444e68dffbe20228f4.jpg',\n",
              " '/content/test/b03f7041962238a7c9d6537e22f9b017.jpg',\n",
              " '/content/test/c978013571258ed6d4637f6e8cc9d6a3.jpg',\n",
              " '/content/test/e0de453c1bffc20c22b072b34b54e50f.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2bRUOfXW6nB",
        "outputId": "d77257d7-9300-4ce0-d84b-f1f759ef53f7"
      },
      "source": [
        "# Let us make sure that the number of images in train set matches with the number of labels in labels.csv, so that we can work on accurately evaluated model\n",
        "import os \n",
        "if len(os.listdir('/content/train')) == len(Train_Filenames) :\n",
        "  print('The no of images in train folder are same as no of labels so we can PROCEED!!')\n",
        "else :\n",
        "  print('The no of images in train folder are not same as no of labels so first look into that before modelling .')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The no of images in train folder are same as no of labels so we can PROCEED!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "3BlmXjs5Riwe",
        "outputId": "76524134-88ae-443e-8454-baeeea345d49"
      },
      "source": [
        "# Lets view an image\n",
        "from IPython.display import Image\n",
        "Image('/content/test/4128bae22183829d2b5fea10effdb0c3.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7mtfJgVjwNY"
      },
      "source": [
        "## Start of 1000 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sHZNKXofNQi"
      },
      "source": [
        "# Set number of images to use for experiments\n",
        "NUM_IMAGES = 1000 #@param{type :\"slider\", min: 1000, max: 10000, step : 1000}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "z38ekurrhD22",
        "outputId": "85c8dbba-1d40-4385-c95e-cc9cfee020a6"
      },
      "source": [
        "X = Train_Filenames\n",
        "Meta_data = train_csv.drop(labels = ['Pawpularity', 'Id'], axis = 1)\n",
        "Y = train_csv['Pawpularity']\n",
        "Meta_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Subject Focus</th>\n",
              "      <th>Eyes</th>\n",
              "      <th>Face</th>\n",
              "      <th>Near</th>\n",
              "      <th>Action</th>\n",
              "      <th>Accessory</th>\n",
              "      <th>Group</th>\n",
              "      <th>Collage</th>\n",
              "      <th>Human</th>\n",
              "      <th>Occlusion</th>\n",
              "      <th>Info</th>\n",
              "      <th>Blur</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Subject Focus  Eyes  Face  Near  ...  Human  Occlusion  Info  Blur\n",
              "0              0     1     1     1  ...      0          0     0     0\n",
              "1              0     1     1     0  ...      0          0     0     0\n",
              "2              0     1     1     1  ...      1          1     0     0\n",
              "3              0     1     1     1  ...      0          0     0     0\n",
              "4              0     0     0     1  ...      0          0     0     0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlW8-cyAfQ4c"
      },
      "source": [
        "# Let us now split data into train and valid using sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Let us split the data\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X[:NUM_IMAGES],\n",
        "                                                    Y[:NUM_IMAGES],\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state= 42) # Same as np.random.seed(42)\n",
        "\n",
        "Meta_train, Meta_valid, Y_trainM, Y_validM = train_test_split(Meta_data[:NUM_IMAGES],\n",
        "                                                    Y[:NUM_IMAGES],\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state= 42)                                         \n",
        "# Let us make sure we are working on exact dimensions of data\n",
        "len(X_train), len(Y_train), len(X_valid), len(Y_valid), len(Meta_train), len(Meta_valid)\n",
        "assert Y_train.all() == Y_trainM.all()\n",
        "assert Y_valid.all() == Y_validM.all()\n",
        "del Y_trainM\n",
        "del Y_validM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcxRHIwNrxpi",
        "outputId": "89798b7b-a0eb-4894-fc32-e37f773063df"
      },
      "source": [
        "print(len(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hCsJdcM3l_7"
      },
      "source": [
        "for i in Meta_data.columns:\n",
        "  if Meta_data[i].any() == 0:\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQh4l23Iw_ma"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPMESEFXwkYZ",
        "outputId": "7e30bee4-7359-448a-decf-c2a7ab30c304"
      },
      "source": [
        "for i in Meta_train.columns:\n",
        "  print(i)\n",
        "  print(Meta_train[i].corr(Y_train))\n",
        "print('no of meta columns', len(Meta_train.columns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject Focus\n",
            "-0.057926307742384255\n",
            "Eyes\n",
            "0.0008825692870834606\n",
            "Face\n",
            "0.007096701814934247\n",
            "Near\n",
            "0.009573249399570605\n",
            "Action\n",
            "0.008875077295543371\n",
            "Accessory\n",
            "-0.01713816820029904\n",
            "Group\n",
            "0.019066942101856832\n",
            "Collage\n",
            "0.030983688351112867\n",
            "Human\n",
            "-0.004416743174041199\n",
            "Occlusion\n",
            "0.0024823341164492032\n",
            "Info\n",
            "-0.050787767036055705\n",
            "Blur\n",
            "0.014859241562109257\n",
            "no of meta columns 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBjVsIiGTVmt"
      },
      "source": [
        "# Image standard size setting up\n",
        "IMAGE_SIZE = 224\n",
        "# Grand Function to carry out all the preprocessing methods on images using different tensorflow methods and classes\n",
        "def preprocessing(image_file) :\n",
        "  '''\n",
        "  The function takes image file path as input all perform some preprocessing on images like :\n",
        "  1. Input image using file_path.\n",
        "  2. Converts image into tensors.\n",
        "  3. Normalize the tensors from 1-255 to 0-1.\n",
        "  4. Resize the image into a standard and common size which is 224*224.\n",
        "  5. Return processed image.\n",
        "  '''\n",
        "  # Step 1: access image using file path\n",
        "  image = tf.io.read_file(image_file)\n",
        "  # Step 2: Converting numpy from of image into tensors with 3 channels\n",
        "  image = tf.image.decode_jpeg(image, channels = 3)\n",
        "  # Step 3: Normalizing the image pixel values.\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  # Step 4: Resizing all images into a standart shape\n",
        "  image = tf.image.resize(image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "  \n",
        "  # Return image\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esqBH6GYsB3m"
      },
      "source": [
        "def meta_preprocessing(meta_data, Y):\n",
        "  meta_data=tf.convert_to_tensor(meta_data)\n",
        "  return meta_data, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzP7zsHodS03"
      },
      "source": [
        "# # A small function to make image tensor and labels into a tuple.\n",
        "# def tupler(image_file, label) :\n",
        "#   '''\n",
        "#   In takes the image file path and then zip the image tensor and label into a tuple.\n",
        "#   '''\n",
        "#   image = preprocessing(image_file)\n",
        "#   return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYlQ7OzYTvM9"
      },
      "source": [
        "# # think of need of tupler file.\n",
        "# # A Grand Function to divide the data either test, train, valid into data batches.\n",
        "# # Shuffling the train set only to make sure the model does not learn form sequence of the set.\n",
        "# BATCH_SIZE = 32\n",
        "# def create_data_batchs(X, y = None, valid_data = False, test_data = False, IsForLatent = False) :\n",
        "#   '''\n",
        "#   The function takes the data as input and prepares data bathcs using tensorflow based on type of set either train or test or valid.\n",
        "#   '''\n",
        "#   # If the data which we need to turn to batchs is test data\n",
        "#   if test_data or IsForLatent:\n",
        "#     print('preparing the test data into batchs')\n",
        "#     # Creating the data as tensors\n",
        "#     data = tf.data.Dataset.from_tensor_slices((tf.constant(X)))\n",
        "#     # Making batchs\n",
        "#     data = data.map(preprocessing).batch(BATCH_SIZE)\n",
        "#   # If the data which we need to turn to batchs is valid data\n",
        "#   elif valid_data and not IsForLatent:\n",
        "#     print('preparing the valid data into batchs')\n",
        "#     # Creating the data as tesors\n",
        "#     data = tf.data.Dataset.from_tensor_slices((tf.constant(X), \n",
        "#                                               tf.constant(y)))\n",
        "#     # Making batchs\n",
        "#     data = data.map(tupler).batch(BATCH_SIZE)\n",
        "#   # If the data which we need to turn to batchs is train data\n",
        "#   elif not IsForLatent:\n",
        "#     print('preparing the train data into batchs')\n",
        "#     # Creating the data as tensors\n",
        "#     data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "#                                               tf.constant(y)))\n",
        "#     # Shuffling the data before making them batchs is to reduce the time for it.\n",
        "#     data = data.shuffle(buffer_size = len(X))\n",
        "#     # Making batchs\n",
        "#     data = data.map(tupler).batch(BATCH_SIZE)\n",
        "#  # Lets return the data ready for experimenting\n",
        "#   return data\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vatRCA_XqeWm"
      },
      "source": [
        "batch_size = 32 # Available into two other places\n",
        "def create_batchs(X, Y= None, image_data = False, Meta_data = False):\n",
        "  if image_data :\n",
        "    print('preparing the image data into batchs for latent space')\n",
        "    # Creating the data as tensors\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X)))\n",
        "    # Making batchs\n",
        "    data = data.map(preprocessing).batch(batch_size)\n",
        "  elif Meta_data:\n",
        "    print('preparing the meta data into batchs for pawpularity')\n",
        "    # Creating the data as tensors\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                              tf.constant(Y)))\n",
        "    # Making batchs\n",
        "    data = data.map(meta_preprocessing).batch(batch_size)\n",
        "  return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDss8He8yOct"
      },
      "source": [
        "### Is there any need for shuffling the meta or image data like done in dog vision project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKbVNgNgyR5j"
      },
      "source": [
        "### Is there any need to remove columns like submission type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2t32imAkvy_",
        "outputId": "59ba6e4c-c898-46ed-f4e7-d3d605f3fc82"
      },
      "source": [
        "train_image_data = create_batchs(X_train, image_data = True)\n",
        "valid_image_data = create_batchs(X_valid, image_data = True)\n",
        "train_meta_data = create_batchs(Meta_train, Y_train, Meta_data = True)\n",
        "valid_meta_data = create_batchs(Meta_valid, Y_valid, Meta_data = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preparing the image data into batchs for latent space\n",
            "preparing the image data into batchs for latent space\n",
            "preparing the meta data into batchs for pawpularity\n",
            "preparing the meta data into batchs for pawpularity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km_wZFa5WPKo",
        "outputId": "46f6664d-9f09-4425-8957-b36b2e2df9bb"
      },
      "source": [
        "train_meta_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 12), (None,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmFnTQ3FWb45"
      },
      "source": [
        "### Define the CNN model ###\n",
        "\n",
        "n_filters = 12 # base number of convolutional filters\n",
        "\n",
        "'''Function to define a standard CNN model'''\n",
        "\n",
        "def make_standard_classifier(n_outputs=1):\n",
        "  Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
        "  BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "  Flatten = tf.keras.layers.Flatten\n",
        "  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    \n",
        "    Conv2D(filters=1*n_filters, kernel_size=5,  strides=2),\n",
        "    BatchNormalization(),\n",
        "    \n",
        "    Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512),\n",
        "    Dense(n_outputs, activation=None),\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "standard_classifier = make_standard_classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mBQF6vLbJ_N"
      },
      "source": [
        "### VAE Reparameterization ###\n",
        "\n",
        "\"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "# Arguments\n",
        "    z_mean, z_logsigma (tensor): mean and log of standard deviation of latent distribution (Q(z|X))\n",
        "# Returns\n",
        "    z (tensor): sampled latent vector\n",
        "\"\"\"\n",
        "def sampling(z_mean, z_logsigma):\n",
        "  # By default, random.normal is \"standard\" (ie. mean=0 and std=1.0)\n",
        "  batch, latent_dim = z_mean.shape\n",
        "  epsilon = tf.random.normal(shape=(batch, latent_dim))\n",
        "  # TODO: Define the reparameterization computation!\n",
        "  # Note the equation is given in the text block immediately above.\n",
        "  z = z_mean + (tf.math.exp(0.5*z_logsigma)*epsilon) # TODO\n",
        "  return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF5KI3NdbK0L"
      },
      "source": [
        "\n",
        "### Define the decoder portion of the DB-VAE ###\n",
        "\n",
        "n_filters = 12 # base number of convolutional filters, same as standard CNN\n",
        "latent_dim = 100 # number of latent variables\n",
        "\n",
        "def make_face_decoder_network():\n",
        "  # Functionally define the different layer types we will use\n",
        "  Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
        "  BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "  Flatten = tf.keras.layers.Flatten\n",
        "  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "  Reshape = tf.keras.layers.Reshape\n",
        "\n",
        "  # Build the decoder network using the Sequential API\n",
        "  decoder = tf.keras.Sequential([\n",
        "    # Transform to pre-convolutional generation\n",
        "    Dense(units=14*14*6*n_filters),  # 4x4 feature maps (with 6N occurances)\n",
        "    Reshape(target_shape=(14, 14, 6*n_filters)),\n",
        "\n",
        "    # Upscaling convolutions (inverse of encoder)\n",
        "    Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),\n",
        "    Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),\n",
        "    Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),\n",
        "    Conv2DTranspose(filters=3, kernel_size=5,  strides=2),\n",
        "  ])\n",
        "\n",
        "  return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_42J-ambCL1"
      },
      "source": [
        "### Defining the VAE loss function ###\n",
        "\n",
        "''' Function to calculate VAE loss given:\n",
        "      an input x, \n",
        "      reconstructed output x_recon, \n",
        "      encoded means mu, \n",
        "      encoded log of standard deviation logsigma, \n",
        "      weight parameter for the latent loss kl_weight\n",
        "'''\n",
        "def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):\n",
        "  # TODO: Define the latent loss. Note this is given in the equation for L_{KL}\n",
        "  # in the text block directly above\n",
        "  #latent_loss = tf.reduce_sum((tf.math.exp(logsigma))+((mu**2)-1-logsigma), axis=0)  # TODO\n",
        "  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)\n",
        "  # TODO: Define the reconstruction loss as the mean absolute pixel-wise \n",
        "  # difference between the input and reconstruction. Hint: you'll need to \n",
        "  # use tf.reduce_mean, and supply an axis argument which specifies which \n",
        "  # dimensions to reduce over. For example, reconstruction loss needs to average \n",
        "  # over the height, width, and channel image dimensions.\n",
        "  # https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean\n",
        "  reconstruction_loss = tf.math.reduce_mean(tf.math.abs(x - x_recon), axis= (1,2,3))# TODO\n",
        "\n",
        "  # TODO: Define the VAE loss. Note this is given in the equation for L_{VAE}\n",
        "  # in the text block directly above\n",
        "  vae_loss = tf.reduce_mean((kl_weight*latent_loss)+reconstruction_loss) # TODO\n",
        "  \n",
        "  return vae_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZFGSCJaE0m_"
      },
      "source": [
        "### Defining and creating the VAE ###\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(VAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "    # Define the number of outputs for the encoder. Recall that we have \n",
        "    # `latent_dim` latent variables, as well as a supervised output for the \n",
        "    # classification.\n",
        "    num_encoder_dims = 2*self.latent_dim # +1\n",
        "\n",
        "    self.encoder = make_standard_classifier(num_encoder_dims)\n",
        "    self.decoder = make_face_decoder_network()\n",
        "\n",
        "  # function to feed images into encoder, encode the latent space, and output\n",
        "  #   classification probability \n",
        "  def encode(self, x):\n",
        "    # encoder output\n",
        "    encoder_output = self.encoder(x)\n",
        "\n",
        "    # # classification prediction\n",
        "    # y_logit = tf.expand_dims(encoder_output[:, 0], -1)\n",
        "    # latent variable distribution parameters\n",
        "    z_mean = encoder_output[:, 0:self.latent_dim] \n",
        "    z_logsigma = encoder_output[:, self.latent_dim:]\n",
        "\n",
        "    return z_mean, z_logsigma\n",
        "\n",
        "  # VAE reparameterization: given a mean and logsigma, sample latent variables\n",
        "  def reparameterize(self, z_mean, z_logsigma):\n",
        "    # TODO: call the sampling function defined above\n",
        "    z = sampling(z_mean, z_logsigma)# TODO\n",
        "    \n",
        "    return z\n",
        "\n",
        "  # Decode the latent space and output reconstruction\n",
        "  def decode(self, z):\n",
        "    # TODO: use the decoder to output the reconstruction\n",
        "    reconstruction = self.decoder(z) # TODO\n",
        "    return reconstruction\n",
        "\n",
        "  # The call function will be used to pass inputs x through the core VAE\n",
        "  def call(self, x): \n",
        "    \n",
        "    # Encode input to a prediction and latent space\n",
        "    z_mean, z_logsigma = self.encode(x)\n",
        "    \n",
        "    # TODO: reparameterization\n",
        "    z =  self.reparameterize(z_mean, z_logsigma) # TODO\n",
        "    \n",
        "    # TODO: reconstruction\n",
        "    recon = self.decode(z) # TODO\n",
        "    \n",
        "    return z_mean, z_logsigma, recon\n",
        "\n",
        "  # # Predict face or not face logit for given input x\n",
        "  # def predict(self, x):\n",
        "  #   y_logit, z_mean, z_logsigma = self.encode(x)\n",
        "  #   return y_logit\n",
        "\n",
        "vae = VAE(latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLNutGiCGfGe",
        "outputId": "99158ee7-6169-433b-d08a-3f1faa8649fc"
      },
      "source": [
        "### Training the VAE ###\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32 # Available into two other places\n",
        "learning_rate = 5e-4\n",
        "latent_dim = 1024\n",
        "\n",
        "# VAE needs slightly more epochs to train since its more complex than \n",
        "# the standard classifier so we use 6 instead of 2\n",
        "num_epochs = 6 # ALL MOST IDEAL EPOCHS EXPERIENCED\n",
        "\n",
        "# instantiate a new VAE model and optimizer\n",
        "vae = VAE(latent_dim)\n",
        "optimizer_vae = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# To define the training operation, we will use tf.function which is a powerful tool \n",
        "#   that lets us turn a Python function into a TensorFlow computation graph.\n",
        "@tf.function\n",
        "def vae_training_step(x):\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Feed input x into vae. Note that this is using the VAE call function!\n",
        "    \n",
        "    z_mean, z_logsigma, x_recon = vae.call(x)\n",
        "    \n",
        "    '''TODO: call the VAE loss function to compute the loss'''\n",
        "    # loss, class_loss = debiasing_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma) # TODO\n",
        "    loss =  vae_loss_function(x, x_recon, z_mean, z_logsigma)\n",
        "  '''TODO: use the GradientTape.gradient method to compute the gradients.\n",
        "     Hint: this is with respect to the trainable_variables of the vae.'''\n",
        "  grads = tape.gradient(loss, vae.trainable_variables) # TODO\n",
        "\n",
        "  # apply gradients to variables\n",
        "  optimizer_vae.apply_gradients(zip(grads, vae.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  for j in range(len(X_train)//batch_size): # Change the range 800 to len(train_csv) after experimenting with small data\n",
        "    (train_images) = next(train_image_data.as_numpy_iterator())\n",
        "    loss = vae_training_step(train_images)\n",
        "  print(loss)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.30913377, shape=(), dtype=float32)\n",
            "tf.Tensor(0.21439308, shape=(), dtype=float32)\n",
            "tf.Tensor(0.17616284, shape=(), dtype=float32)\n",
            "tf.Tensor(0.16838518, shape=(), dtype=float32)\n",
            "tf.Tensor(0.16166815, shape=(), dtype=float32)\n",
            "tf.Tensor(0.16084795, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IblpT4Hq1EC"
      },
      "source": [
        "\n",
        "# # get training faces from data loader\n",
        "# all_faces = loader.get_all_train_faces()\n",
        "\n",
        "# if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "# # The training loop -- outer loop iterates over the number of epochs\n",
        "# for i in range(num_epochs):\n",
        "\n",
        "#   IPython.display.clear_output(wait=True)\n",
        "#   print(\"Starting epoch {}/{}\".format(i+1, num_epochs))\n",
        "\n",
        "#   # Recompute data sampling proabilities\n",
        "#   '''TODO: recompute the sampling probabilities for debiasing'''\n",
        "#   p_faces = get_training_sample_probabilities(all_faces, dbvae) # TODO\n",
        "  \n",
        "#   # get a batch of training data and compute the training step\n",
        "#   for j in tqdm(range(loader.get_train_size() // batch_size)):\n",
        "#     # load a batch of data\n",
        "#     (x, y) = loader.get_batch(batch_size, p_pos=p_faces)\n",
        "#     # loss optimization\n",
        "#     loss = debiasing_train_step(x, y)\n",
        "    \n",
        "#     # plot the progress every 200 steps\n",
        "#     if j % 500 == 0: \n",
        "#       mdl.util.plot_sample(x, y, dbvae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYTRJ2YHgCFN"
      },
      "source": [
        "# (valid_images) = next(valid_image_data.as_numpy_iterator())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipz7LDFtuArB"
      },
      "source": [
        "# z_mean, z_logsigma, x_recon = vae.call(valid_images)\n",
        "# loss =  vae_loss_function(valid_images, x_recon, z_mean, z_logsigma)\n",
        "# print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlyQaErjLKGR"
      },
      "source": [
        "# def get_Z(images):\n",
        "#   # vae_training_step(images)\n",
        "#   z_mean, z_logsigma = vae.encode(images)\n",
        "#   z = vae.reparameterize(z_mean, z_logsigma)\n",
        "#   return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg87EIQ0sEGU"
      },
      "source": [
        "# (valid_meta, valid_labels) = next(valid_meta_data.as_numpy_iterator())\n",
        "# valid_meta.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50zN4WCApeyA"
      },
      "source": [
        "# z_mean, z_logsigam = vae.encode(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KJLIewgHyh-"
      },
      "source": [
        "# z = vae.reparameterize(z_mean, z_logsigam)\n",
        "# print(z.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VARC1WD4T8br"
      },
      "source": [
        "## Working for z and meta data and designing a model neural network for pawpularity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIEGn1N9w5QT"
      },
      "source": [
        "# def tf_concatenator(z, meta_data, labels):\n",
        "#   combined_data = tf.concat([z, meta_data], axis= 1)\n",
        "#   return combined_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmTZIEPjxYtH"
      },
      "source": [
        "# print(tf_concatenator(z, valid_meta, valid_labels).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mOzbZ2cLppo"
      },
      "source": [
        "def Meta_model(output_dim):\n",
        "  meta_model = tf.keras.Sequential([\n",
        "                        tf.keras.layers.Dense(units = 1024, activation = 'relu'),\n",
        "                        tf.keras.layers.Dense(units = 512, activation = 'relu'),\n",
        "                        tf.keras.layers.Dense(units = 248, activation = 'relu'),\n",
        "                        tf.keras.layers.Dense(units = 100, activation = 'relu'),\n",
        "                        tf.keras.layers.Dense(units = output_dim, activation= None)\n",
        "  ])\n",
        "  return meta_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgw-25kaja4s"
      },
      "source": [
        "### do we need to have a probablistic value or a deterministic value for last dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ6KGenFYhlO"
      },
      "source": [
        "# meta_model = Final_pawpularity()\n",
        "# meta_model.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1gj31L_YkLx"
      },
      "source": [
        "# meta_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8oyHcmPasYr"
      },
      "source": [
        "### Defining and creating the VAE ###\n",
        "output_dim = 1\n",
        "class PAW(tf.keras.Model):\n",
        "  def __init__(self, output_dim):\n",
        "    super(PAW, self).__init__()\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    # Define the number of outputs for the encoder. Recall that we have \n",
        "    # `latent_dim` latent variables, as well as a supervised output for the \n",
        "    # classification.\n",
        "    # num_encoder_dims = 2*self.latent_dim # +1\n",
        "\n",
        "    self.meta_model = Meta_model(output_dim)\n",
        "    # self.decoder = make_face_decoder_network()\n",
        "\n",
        "  # function to feed images into encoder, encode the latent space, and output\n",
        "  #   classification probability \n",
        "  def get_z(self, images):\n",
        "    z_mean, z_logsigma = vae.encode(images)\n",
        "    z = vae.reparameterize(z_mean, z_logsigma)\n",
        "    return z\n",
        "\n",
        "    # # # classification prediction\n",
        "    # # y_logit = tf.expand_dims(encoder_output[:, 0], -1)\n",
        "    # # latent variable distribution parameters\n",
        "    # z_mean = encoder_output[:, 0:self.latent_dim] \n",
        "    # z_logsigma = encoder_output[:, self.latent_dim:]\n",
        "\n",
        "    # return z_mean, z_logsigma\n",
        "\n",
        "  def tf_concatenator(self, z, meta_data):\n",
        "    meta_data = tf.cast(meta_data, dtype=tf.float32)\n",
        "    combined_data = tf.concat([z, meta_data], axis= 1)\n",
        "    return combined_data\n",
        "\n",
        "  # # VAE reparameterization: given a mean and logsigma, sample latent variables\n",
        "  # def reparameterize(self, z_mean, z_logsigma):\n",
        "  #   # TODO: call the sampling function defined above\n",
        "  #   z = sampling(z_mean, z_logsigma)# TODO\n",
        "    \n",
        "  #   return z\n",
        "\n",
        "  # # Decode the latent space and output reconstruction\n",
        "  # def decode(self, z):\n",
        "  #   # TODO: use the decoder to output the reconstruction\n",
        "  #   reconstruction = self.decoder(z) # TODO\n",
        "  #   return reconstruction\n",
        "\n",
        "  # The call function will be used to pass inputs x through the core VAE\n",
        "  def call(self, images, meta_data): \n",
        "    \n",
        "    # Encode input to a prediction and latent space\n",
        "    z = self.get_z(images)\n",
        "    combined_x = self.tf_concatenator(z, meta_data)\n",
        "    predictions = self.meta_model(combined_x)\n",
        "    return predictions\n",
        "  # # Predict face or not face logit for given input x\n",
        "  # def predict(self, x):\n",
        "  #   y_logit, z_mean, z_logsigma = self.encode(x)\n",
        "  #   return y_logit\n",
        "\n",
        "paw = PAW(output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3eF_7-C-VC_"
      },
      "source": [
        "def paw_loss_function(predictions, y_true):\n",
        "  loss = tf.sqrt(tf.reduce_mean((tf.cast(y_true, dtype= tf.float32) - tf.squeeze(predictions))**2))\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKgzBZwk7Tpm",
        "outputId": "e551aa6b-9190-4966-97a0-d085e760b651"
      },
      "source": [
        "### Training the VAE ###\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32 # Available into two other places\n",
        "learning_rate = 5e-4\n",
        "output_dim = 1\n",
        "\n",
        "# VAE needs slightly more epochs to train since its more complex than \n",
        "# the standard classifier so we use 6 instead of 2\n",
        "num_epochs = 2 # ALL MOST IDEAL EPOCHS EXPERIENCED\n",
        "\n",
        "# instantiate a new DB-VAE model and optimizer\n",
        "paw = PAW(output_dim)\n",
        "optimizer_paw = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# To define the training operation, we will use tf.function which is a powerful tool \n",
        "#   that lets us turn a Python function into a TensorFlow computation graph.\n",
        "@tf.function\n",
        "def paw_training_step(images, meta_data, y_true):\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Feed input x into dbvae. Note that this is using the VAE call function!\n",
        "    \n",
        "    predictions = paw.call(images, meta_data)\n",
        "    \n",
        "    '''TODO: call the VAE loss function to compute the loss'''\n",
        "    # loss, class_loss = debiasing_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma) # TODO\n",
        "    loss =  paw_loss_function(predictions, y_true)\n",
        "  '''TODO: use the GradientTape.gradient method to compute the gradients.\n",
        "     Hint: this is with respect to the trainable_variables of the dbvae.'''\n",
        "  grads = tape.gradient(loss, paw.trainable_variables) # TODO\n",
        "\n",
        "  # apply gradients to variables\n",
        "  optimizer_paw.apply_gradients(zip(grads, paw.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  for j in range(len(X_train)//batch_size): # Change the range 800 to len(train_csv) after experimenting with small data\n",
        "    (train_metas, y_true) = next(train_meta_data.as_numpy_iterator())\n",
        "    (train_images) = next(train_image_data.as_numpy_iterator())\n",
        "    loss = paw_training_step(train_images, train_metas, y_true)\n",
        "    print(loss)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(32.005344, shape=(), dtype=float32)\n",
            "tf.Tensor(30.31035, shape=(), dtype=float32)\n",
            "tf.Tensor(28.619122, shape=(), dtype=float32)\n",
            "tf.Tensor(26.782146, shape=(), dtype=float32)\n",
            "tf.Tensor(24.668812, shape=(), dtype=float32)\n",
            "tf.Tensor(22.034243, shape=(), dtype=float32)\n",
            "tf.Tensor(19.251377, shape=(), dtype=float32)\n",
            "tf.Tensor(16.056482, shape=(), dtype=float32)\n",
            "tf.Tensor(12.8138275, shape=(), dtype=float32)\n",
            "tf.Tensor(10.936364, shape=(), dtype=float32)\n",
            "tf.Tensor(11.534652, shape=(), dtype=float32)\n",
            "tf.Tensor(14.0390415, shape=(), dtype=float32)\n",
            "tf.Tensor(13.803701, shape=(), dtype=float32)\n",
            "tf.Tensor(13.881571, shape=(), dtype=float32)\n",
            "tf.Tensor(13.204272, shape=(), dtype=float32)\n",
            "tf.Tensor(11.317527, shape=(), dtype=float32)\n",
            "tf.Tensor(9.994785, shape=(), dtype=float32)\n",
            "tf.Tensor(9.693277, shape=(), dtype=float32)\n",
            "tf.Tensor(10.776836, shape=(), dtype=float32)\n",
            "tf.Tensor(10.677, shape=(), dtype=float32)\n",
            "tf.Tensor(11.177558, shape=(), dtype=float32)\n",
            "tf.Tensor(11.198285, shape=(), dtype=float32)\n",
            "tf.Tensor(11.135946, shape=(), dtype=float32)\n",
            "tf.Tensor(10.841631, shape=(), dtype=float32)\n",
            "tf.Tensor(9.567171, shape=(), dtype=float32)\n",
            "tf.Tensor(9.995809, shape=(), dtype=float32)\n",
            "tf.Tensor(10.345639, shape=(), dtype=float32)\n",
            "tf.Tensor(9.223658, shape=(), dtype=float32)\n",
            "tf.Tensor(10.451297, shape=(), dtype=float32)\n",
            "tf.Tensor(9.595111, shape=(), dtype=float32)\n",
            "tf.Tensor(9.8888, shape=(), dtype=float32)\n",
            "tf.Tensor(9.779677, shape=(), dtype=float32)\n",
            "tf.Tensor(9.811351, shape=(), dtype=float32)\n",
            "tf.Tensor(8.83996, shape=(), dtype=float32)\n",
            "tf.Tensor(8.801191, shape=(), dtype=float32)\n",
            "tf.Tensor(9.453825, shape=(), dtype=float32)\n",
            "tf.Tensor(9.29889, shape=(), dtype=float32)\n",
            "tf.Tensor(8.653225, shape=(), dtype=float32)\n",
            "tf.Tensor(9.389554, shape=(), dtype=float32)\n",
            "tf.Tensor(9.032969, shape=(), dtype=float32)\n",
            "tf.Tensor(8.5929, shape=(), dtype=float32)\n",
            "tf.Tensor(8.585861, shape=(), dtype=float32)\n",
            "tf.Tensor(8.599821, shape=(), dtype=float32)\n",
            "tf.Tensor(8.216119, shape=(), dtype=float32)\n",
            "tf.Tensor(8.436706, shape=(), dtype=float32)\n",
            "tf.Tensor(9.612766, shape=(), dtype=float32)\n",
            "tf.Tensor(9.929234, shape=(), dtype=float32)\n",
            "tf.Tensor(8.23698, shape=(), dtype=float32)\n",
            "tf.Tensor(8.544553, shape=(), dtype=float32)\n",
            "tf.Tensor(8.179777, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmMWdJ-BUJk1"
      },
      "source": [
        "### Valid predicitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh93WPC62UVS",
        "outputId": "1a26c078-5c51-48b5-8fe1-a70cde89625a"
      },
      "source": [
        "# total valid set\n",
        "for i in range(len(X_valid)//batch_size):\n",
        "  (valid_metas, y_true) = next(valid_meta_data.as_numpy_iterator())\n",
        "  (valid_images) = next(valid_image_data.as_numpy_iterator())\n",
        "  predictions = paw.call(valid_images, valid_metas)\n",
        "  mse = paw_loss_function(predictions, y_true)\n",
        "  print(mse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(22.186758, shape=(), dtype=float32)\n",
            "tf.Tensor(22.910269, shape=(), dtype=float32)\n",
            "tf.Tensor(21.320229, shape=(), dtype=float32)\n",
            "tf.Tensor(22.457972, shape=(), dtype=float32)\n",
            "tf.Tensor(22.306618, shape=(), dtype=float32)\n",
            "tf.Tensor(22.142687, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnZHA5bbDPHO"
      },
      "source": [
        "(valid_metas, y_true) = next(valid_meta_data.as_numpy_iterator())\n",
        "(valid_images) = next(valid_image_data.as_numpy_iterator())\n",
        "predictions = paw.call(valid_images, valid_metas)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0Ctdq7olBye"
      },
      "source": [
        "z_mean, z_logsigma, x_recon = vae.call(valid_images)\n",
        "latent_loss =  vae_loss_function(valid_images, x_recon, z_mean, z_logsigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-QgI42ZflG5",
        "outputId": "0c14d764-16a8-4b17-d111-7760adfaac7f"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
              "array([[29.584097],\n",
              "       [33.08997 ],\n",
              "       [29.871193],\n",
              "       [27.881334],\n",
              "       [34.458874],\n",
              "       [25.146557],\n",
              "       [28.595871],\n",
              "       [35.072243],\n",
              "       [23.034458],\n",
              "       [43.79861 ],\n",
              "       [29.4834  ],\n",
              "       [33.94175 ],\n",
              "       [27.628723],\n",
              "       [27.844177],\n",
              "       [31.988972],\n",
              "       [29.713047],\n",
              "       [34.639   ],\n",
              "       [23.773382],\n",
              "       [30.824936],\n",
              "       [19.475529],\n",
              "       [29.613195],\n",
              "       [31.190502],\n",
              "       [26.833183],\n",
              "       [31.443487],\n",
              "       [24.549286],\n",
              "       [22.595127],\n",
              "       [28.173126],\n",
              "       [22.033741],\n",
              "       [38.75439 ],\n",
              "       [25.365156],\n",
              "       [26.497278],\n",
              "       [26.88113 ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "826B1URYz8P2",
        "outputId": "a22b1ad5-47e5-43e2-b2ef-f7227cc81ee0"
      },
      "source": [
        "y_true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 17,  30,  26,  72,  29,  35,  26,  71,   4,  39,  28,  26,  49,\n",
              "        32, 100,  45,  59,  48,  34,  73,  49,  23,  31,  47,  26,  38,\n",
              "        43,   9,  35,  29,  50,  36])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtfCGsP5gAxd",
        "outputId": "31d90d32-81f8-4a21-dfbc-baaf2263d34e"
      },
      "source": [
        "mse = paw_loss_function(predictions, y_true)\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=21.928286>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne23TMBaU16q"
      },
      "source": [
        "### For test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6qhl3QbDTzl"
      },
      "source": [
        "test_image_data = create_batchs()\n",
        "(test_images, test_metas) = next\n",
        "(test_image_data.as_numpy_iterator())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}